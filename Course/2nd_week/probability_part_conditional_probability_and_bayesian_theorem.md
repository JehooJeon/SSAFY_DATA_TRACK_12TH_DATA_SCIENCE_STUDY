# 조건부 확률 & 베이지안 통계학

> 통계의 중요한 개념인 조건부 확률과 베이지안 통계학을 위해 필요한 다양한 개념들
조건부 확률과 베이지안 통계학은 왜 배우는가?
데이터셋을 통한 파이썬 실습
> 

<aside>
3️⃣

**베이지안 이론을 위한 세 가지 재료**

---

- **조건부확률**
    - 곱의 법칙
- **독립**
- **전확률법칙**
</aside>

## 그래도 한 번 보면 좋지 않을까?

---

<aside>
📢

**확률 개념 오프닝**

---

- **통계적 실험  Random Experiment**
    - 자료가 **확률**에 바탕을 두는 실험
    - 우리가 관찰하는 모든 것은 통계적 실험에 가깝다
- **표본 공간  Sample Space**
    - 통계적 실험에서 발생 가능한 **모든** **가능한** 결과들의 집합
- **사건  Event**
    - 표본 공간의 **부분 집합**
    - 표본 공간에서 특정한 조건을 만족하는 결과를 모아놓은 집합
    - 전사건, 공사건, 여사건, 합사건, 곱사건, 배반사건
- **확률  Probability**
    - **수학적 확률** : 어떤 사건이 일어나는 경우 / 표본 공간, 기댓값
    - **통계적 확률** : 직접 해보니, 그렇게 나왔어, 빈도 수
        - **대수의 법칙** : 시행 횟수가 무한히 크면 수학적 확률 $\approx$ 통계적 확률
    - **베이지안 확률** : 앞으로 배울 것

---

</aside>

<aside>
❓

**remind 예시**

---

- 동전 3개를 던지는 **통계적 실험**을 시행했을 때, **표본 공간**은 다음과 같다.

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image.png)

- 이때 앞면의 개수가 두 개인 **사건**은 **{ 앞앞뒤, 앞뒤앞, 뒤앞앞 }**
- 이 사건이 발생할 **확률**은?
P(앞면의 개수가 두 개인 사건) = **3 / 8**
</aside>

<aside>
➕

**추가적인 개념**

---

- **합의 법칙 & 곱의 법칙**
    - 합의 법칙 : +, or, 또는, 동시 x, 합집합, union
        - 모두 앞일 확률 + 모두 뒤일 확률 = 2 / 8
            - $P(A \cup B) = P(A) + P(B) - P(A \cap B)$
            - $P(A \cup B) = P(A) + P(B)$
    - 곱의 법칙 : x, and, 그리고, 동시, 교집합, intersection
        - 경우의 수
- **배반 & 독립**
    - 배반 : 동시에 일어나지 않는 사건, 서로소
        - $A \cap B = \varnothing$
    - 독립 : 사건 A의 발생이 사건 B의 확률에 영향을 주지 않음
        - $P(A \cap B) = P(A) \times P(B)$
- **순열 & 조합**
    - 순열 : 순서 고려
    - 조합 : 순서 안 고려
</aside>

![2021-dollar-penny-kangaroo-unc.jpg](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/2021-dollar-penny-kangaroo-unc.jpg)

> 동전을 두 번 던졌을 때, 두 번 모두 앞면이 나올 확률은 얼마나 될까?
동전을 두 번 던졌을 때, **첫 번째에 앞면이 나왔다면**, 
두 번 모두 앞면이 나올 확률은 얼마나 될까?
> 

## 조건부 확률

---

### 조건부 확률이란?

---

![images.jpg](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/images.jpg)

$$
P(B|A) = \frac {P(A \cap B)} {P(A)}
$$

$$
{P(A \cap B)} =  P(B|A)P(A)
$$

<aside>
💡

조건부확률의 **조건**은 **표본 공간 S를 바꾼다**.

---

- 위의 예시에서, S = { HH, HT, TH, TT } 이 S’ = { **H**H, **H**T } 로 바뀌었다.
- 표본 공간이 작아지는 방향으로 바뀐다.
</aside>

<aside>
💡

조건부확률 $P(B|A)$는 **입력변수 A**에 대해 **정답이 B일 확률**을 의미한다.

---

- 어떤 사건 A는 발생할 수도, 발생하지 않을 수도 있다.
- 그 중 사건 A가 발생했을 때, 사건 B가 발생할 확률을 구해야 한다.
</aside>

<aside>
💡

사건 A와 B가 **동시에 발생할 확률**과는 다르다.

---

- 사건 A와 B가 동시에 발생할 확률은 표본 공간이 변하지 않는다.
    - $\frac {P(A \cap B)} {P(S)}$
</aside>

![Venn0001.svg.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/Venn0001.svg.png)

### 조건부 확률이 왜 필요하지?

---

<aside>
💡

> **조건부확률**은 머신러닝 모델이 데이터에서 패턴을 학습하고, 특정 조건 하에서 예측을 수행하는 데 필수적인 개념이다.

이를 통해 모델이 더 정확하고 신뢰성 있는 예측을 할 수 있게 된다. 

따라서 머신러닝을 제대로 이해하고 활용하려면 **조건부확률**을 잘 이해하는 것이 매우 중요하다.
> 
</aside>

<aside>
💡

> **로지스틱 회귀**의 **선형모델**과 **소프트맥스 함수**의 결합은 데이터에서 추출된 패턴을 기반으로 확률을 해석하는데 사용된다.

**분류 문제**에서 $softmax(W\phi +b)$는 데이터 x로부터 추출된 특징패턴 $\phi(x)$과 가중치행렬 W를 통해 조건부확률 $P(y|x)$을 계산한다.

**회귀 문제**의 경우 조건부기대값 $\mathbb{E}[y|x]$ 을 추정한다.

**딥러닝**은 **다층신경망**을 사용하여 데이터로부터 특징패턴 $\phi$을 추출한다.
> 
</aside>

### 조건부 확률 예제

---

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%201.png)

### 독립 사건

---

- A가 B의 발생에 영향을 주지 못한다.
- 이 때, 곱의 법칙에 변화가 생긴다.

$$
{P(A \cap B)} =  P(B|A)P(A)
$$

$$
{P(A \cap B)} =  P(A)P(B)
$$

### 독립성 판별 예제

---

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%202.png)

### 몬티홀 문제

---

![Asset-4-1024x438.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/Asset-4-1024x438.png)

```python
당신이 한 게임 쇼에서 3개의 문 중에 하나를 고를 수 있는 상황이라고 가정하자. 
한 문 뒤에는 자동차가, 다른 두 문 뒤에는 염소가 있다. 
당신이 1번 문을 고르자, 문 뒤에 무엇이 있는지 아는 사회자는 3번 문을 열어서 염소를 보여줬다. 
그리고는 "2번 문으로 바꾸시겠습니까?"라고 물었다. 
이 상황에서, 당신의 선택을 바꾸는 게 유리할까?
```

<aside>
❗

**핵심 규칙**

---

- 닫혀 있는 문 3개가 있다.
- 한 문 뒤에는 상품(= 🚗)이 있고, 나머지 두 문은 꽝(= 🐐)이다.
- 참가자는 이 3가지 문 중 하나를 골라야 상품을 얻을 수 있다.
- 참가자가 문 하나를 고르면, 이미 상품이 어딨는지 알고 있는 사회자는 남은 2가지 문 중에 하나를 열고 그게 '꽝'이라는 사실을 밝힌다.
- 여기서 **참가자에게 다른 문으로 바꿀 수 있는 기회가 주어진다.**
</aside>

<aside>
❗

**전제**

---

- 사회자는 자동차가 어느 문 뒤에 있는지 알고 있다.
- 사회자는 이 정보에 기반해서 추가적으로 문을 열어줘야 한다.
- 사회자는 내가 고른 문은 열지 않는다.
- 사회자는 염소가 있는 문 중, 열어주는 문은 임의로 선택한다.
</aside>

<aside>
💡

**이 전제가 실제 확률에 영향을 미치게 된다.** 

</aside>

### **몬티홀 - 문을 바꾸는 않는 경우의 확률**

---

- 문을 바꾸지 않는 경우는, 사회자가 문 B를 열었을 때, 자동차가 문 A에 있을 확률

- **사건 정의:**
    - A : 자동차가 문 A에 있을 사건
    - B : 사회자가 문 B를 열었다는 사건
- **조건부 확률 :**

$$
P(A∣B)= \frac {P(A∩B)} {P(B)} 
$$

- **계산 과정:**
    1. **전체 확률 P(B) 계산**
        - 자동차가 문 A에 있을 확률: $\frac {1} {3}$
        - 자동차가 문 B에 있을 확률: $\frac {1} {3}$
        - 자동차가 문 C에 있을 확률: $\frac {1} {3}$
            
            
        
        사회자가 문 B를 열었을 확률은 자동차가 문 A나 문 C에 있을 때만 고려
        
        - 자동차가 문 A에 있을 경우: 사회자는 문 B나 문 C 중 염소가 있는 문을 열 수 있음.
        문 B를 여는 확률은 $\frac {1} {2}$
        - 자동차가 문 B에 있을 경우: 사회자는 문 C를 열어야 한다. 문 B를 여는 확률은 0.
        - 자동차가 문 C에 있을 경우: 사회자는 문 B를 열어야 한다. 문 B를 여는 확률은 1.
        
        따라서,
        
        $$
        P(B) = \frac{1}{3} \times \frac{1}{2} + \frac{1}{3} \times 0 + \frac{1}{3} \times 1 = \frac{1}{6} + 0 + \frac{1}{3} = \frac{1}{2}
        $$
        
    2. **교집합 확률 P(A∩B) 계산:**
        - 자동차가 문 A에 있을 때 문 B를 여는 경우의 수를 고려한다.
        - 문 A에 자동차가 있을 확률은 $\frac {1} {3}$, 이때 문 B를 여는 확률은 $\frac {1} {2}$
        
        따라서,
        
        $$
        P(A∩B)=P(A)×P(B∣A)=\frac {1} {3} × \frac {1} {2} = \frac {1} {6}
        $$
        
    3. **조건부 확률 P(A∣B) 계산:**
        
        $$
        P(A∣B)=\frac{P(A \cap B)}{P(B)} = \frac{\frac{1}{6}}{\frac{1}{2}} = \frac{1}{3}
        $$
        
    
    <aside>
    🥲
    
    **내가 문을 바꾸지 않았을 때, 자동차를 얻게 될 확률은** $\frac {1} {3}$
    
    </aside>
    

### 몬티홀 - 문을 바꾸는 경우의 확률

---

- 문을 바꾸는 경우는, 사회자가 문 B를 열었을 때, 자동차가 문 C에 있을 확률

- **사건 정의:**
    - C : 자동차가 문 C에 있을 사건
    - B : 사회자가 문 B를 열었다는 사건
- **조건부 확률:**

$$
P(C∣B)= \frac {P(C∩B)} {P(B)} 
$$

- **계산 과정:**
    1. **교집합 확률 P(C∩B) 계산:**
        - 자동차가 문 C에 있을 때 문 B를 여는 경우의 수를 고려합니다.
        - 자동차가 문 C에 있을 확률은 $\frac {1} {3}$, 이때 문 B를 여는 확률은 1입니다.
        
        따라서,
        
        $$
        P(C∩B)=P(C)×P(B∣C)=  \frac{1}{3} \times 1 = \frac{1}{3}
        $$
        
    2. **조건부 확률 P(C∣B) 계산:**
    
    $$
    
    P(C∣B)=
    \frac{P(C \cap B)}{P(B)} = \frac{\frac{1}{3}}{\frac{1}{2}} = \frac{2}{3}
    
    $$
    
    <aside>
    🥲
    
    **내가 문을 바꿨을 때, 자동차를 얻게 될 확률은** $\frac {2} {3}$
    
    </aside>
    

### 결론

---

- 문을 유지할 경우 자동차를 얻을 확률: $\frac {1} {3}$
- 문을 바꿀 경우 자동차를 얻을 확률: $\frac {2} {3}$

<aside>
💡

**사회자가 문을 열어주는 행위가 조건이 되어 표본 공간이 바뀌었다.**

</aside>

<aside>
✅

**사회자가 문을 열어준다는 사건 B가 발생해서, 조건부 확률을 적용하게 되었고 표본 공간이 바뀌었다.**

</aside>

<aside>
💥

**문을 바꾸는 행위는, 처음 선택할 때 염소와 차의 확률을 바꾸는 행위이다.**

</aside>

### 조건부 확률 예제 코드 - 몬티홀 문제

---

```python
import random

def monty_hall_simulation(switch, trials = 10000):

    wins = 0
    
    for _ in range(trials):
        # 3개의 문 중 하나를 무작위로 선택합니다 (0, 1, 2)
        car_position = random.randint(0, 2)
        player_choice = random.randint(0, 2)

        # 사회자가 열 수 있는 문들을 찾습니다
        remaining_doors = []
        for door in range(3):
            if door != player_choice and door != car_position:
                remaining_doors.append(door)
        
        monty_opens = random.choice(remaining_doors)

        # 플레이어가 문을 바꾼다면
        if switch:
            for door in range(3):
                if door != player_choice and door != monty_opens:
                    player_choice = door
                    break

        # 플레이어가 차를 얻는 경우
        if player_choice == car_position:
            wins += 1

    # 이길 확률을 반환합니다
    return wins / trials

# 시뮬레이션 실행
no_switching = monty_hall_simulation(switch=False)
switching = monty_hall_simulation(switch=True)

print(f"바꾸지 않는 경우 이길 확률: {no_switching:.2%}")
print(f"바꾸는 경우 이길 확률: {switching:.2%}")

```

## 전확률법칙

---

- P(A)를 구하기 위해 A의 경우의 수를 세는 것이 아니라, 부분적인 정보를 조합하여 P(A)를 얻는다.
    
    ![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%203.png)
    

- A는 Bn과 A의 교집합들의 합으로 나타낼 수 있다.
    
    ![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%204.png)
    
- 수식 변경

### 예제

---

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%205.png)

## 베이지안 확률

---

### 빈도 확률 vs 베이지안 확률

---

<aside>
💡

**확률의 고전적 정의**

---

- 수학적 확률
- 통계적 확률 (빈도 확률)
</aside>

<aside>
💡

**베이지안 확률**

---

- 반복할 수 없는 사건은 무수히 많고, **빈도 확률**은 이러한 사건에 적용할 수 없음
- 일어나지 않은 일에 대한 확률을 **불확실성**의 개념으로 보고,
사건과 관련 있는 여러 확률을 이용해 새롭게 일어날 사건을 추정하는 것이
**베이지안 확률**
</aside>

<aside>
💡

**모집단을 변하지 않는 대상**으로 규정하는 **기존 통계학**과 달리, **베이즈 통계학**에서는 **모집단을 미리 확정짓지 않는다**.

</aside>

## 베이즈 정리

---

<aside>
💡

어떤 사건이 서로 배반하는 원인 둘에 의해 일어난다고 할 때, 실제 사건이 일어났을 때 이것이 두 원인 중 하나일 확률을 구하는 정리를 베이즈의 정리라고 한다.

</aside>

<aside>
💡

이전의 경험과 현재의 증거를 토대로 어떤 사건의 확률을 추론하는 과정

</aside>

<aside>
💡

베이즈 정리는, 종속적(의존적) 관계에 놓인 사건들을 기반으로 확률을 구한다.

</aside>

<aside>
💡

베이즈 정리는 **기존의 지식(사전 확률)**을 업데이트하기 위해 **새로운 정보(데이터)**를 반영하여 **어떤 사건의 확률**(**사후 확률)**을 구하는 방법으로, **불확실성**을 다루는 다양한 상황에서 유용하게 사용됩니다.

</aside>

### 베이즈 정리는 왜 필요해요?

---

```jsx
당신은 어떤 화산이 가까운 미래에 폭발할 확률을 계산하는 임무를 받았다.
이 화산은 아주 예전에 몇 번 폭발한 적이 있다고 한다.
정보가 부족한데... 난 어떻게 확률을 구해야하지?
그러던 중 대수의 법칙이 떠올라, 동전을 던지듯 화산이 굉장히 많이 터질때까지 기다리기로 했다.
```

<aside>
💡

**아하! 베이즈적인 접근이 필요해**

---

이런 반복할 수 없는 사건들은 전통적인 확률로는 신뢰할만한 값을 얻기가 어렵다.

때문에 일어나지 않은 **화산 폭발**이라는 사건에 대한 확률을 **사건과 관련된 여러 확률**을 이용해 새롭게 추정하는 것이 베이즈 정리를 이용한 것이다.

</aside>

### 예시

---

<aside>
1️⃣

**화산 폭발 예측**

---

- **상황**: 특정 화산의 폭발 확률을 예측해야 한다고 가정하자. 이미 수백 년 동안 이 화산이 폭발한 빈도에 대한 데이터를 가지고 있다(**사전 확률**). 새로운 정보로 지진, 온도 변화, 가스 배출 등의 데이터를 실시간으로 수집할 수 있다.
- **베이지안 접근**: **새로운 데이터**가 관측될 때마다 기존의 폭발 확률을 업데이트하여 현재 상황에서의 폭발 확률을 계산할 수 있다. 예를 들어, 최근 지진 활동이 증가했다면, 베이지안 정리를 통해 이 정보를 반영하여 화산 폭발의 **사후 확률**을 계산할 수 있다.
</aside>

<aside>
2️⃣

**의료 진단**

---

- **상황**: 특정 질병에 대한 환자의 진단을 예측해야 한다. 해당 질병의 발생 확률(예를 들어, 인구 통계에 기반한 **사전 확률**)이 있으며, 환자의 특정 증상이나 검사 결과가 있다.
- **베이지안 접근**: 특정 증상이 나타났을 때 그 환자가 질병에 걸렸을 확률을 계산할 수 있다. 예를 들어, 특정 질병에 대한 검사 결과가 양성일 때, 해당 검사 결과와 일반적인 질병 발병률(**사전 확률**)을 바탕으로 그 환자가 실제로 질병에 걸렸을 확률(**사후 확률**)을 업데이트할 수 있다.
</aside>

<aside>
3️⃣

**주식 시장 예측**

---

- **상황**: 주식의 가격이 특정 요인들에 의해 영향을 받는 상황을 고려한다. 이미 주식 가격에 영향을 미칠 수 있는 요인들에 대한 과거 데이터를 가지고 있으며(**사전 확률**), 새로 발표된 경제 지표나 기업의 실적 보고서 같은 새로운 정보가 있다.
- **베이지안 접근**: 새로운 경제 지표가 발표될 때마다, 이 정보를 반영하여 주식 가격이 상승하거나 하락할 확률을 업데이트할 수 있다. 예를 들어, 금리 인상이 발표되면, 이 정보를 바탕으로 주식 시장의 반응을 베이지안 정리로 계산할 수 있다.
</aside>

<aside>
4️⃣

**스팸 메일 필터링**

---

- **상황**: 이메일이 스팸인지 아닌지 판별하는 문제. 과거 데이터를 통해 특정 단어나 패턴이 포함된 이메일이 스팸일 확률(사전 확률)을 가지고 있다.
- **베이지안 접근**: 새로운 이메일이 도착했을 때, 그 이메일에 특정 단어가 포함되어 있거나 특정 패턴을 보이는 경우, 이 정보를 바탕으로 그 이메일이 스팸일 확률(사후 확률)을 계산할 수 있다.
</aside>

<aside>
5️⃣

**범죄 수사**

---

- **상황**: 특정 사건에 대한 용의자가 여러 명 있을 때, 각 용의자가 범인일 확률을 계산한다. 기존의 증거와 용의자의 전과 기록(사전 확률)이 있으며, 새로운 증거가 발견될 때마다 확률을 업데이트한다.
- **베이지안 접근**: 새로 발견된 증거(예: DNA, 목격자의 진술 등)를 바탕으로 용의자가 범인일 확률을 업데이트하여 수사의 방향을 정하는 데 도움을 줄 수 있다.
</aside>

---

<aside>
💡

기존 사건들의 확률**(사전 확률)**을 **데이터**를 바탕으로 업데이트하여 조건부 확률**(사후 확률)**을 구하기 위해

</aside>

<aside>
💡

기존 사건들의 확률을 알지 못한다면 베이즈 정리는 의미를 가지지 못한다.

</aside>

<aside>
💡

최근, 늘어난 **데이터의 양** 덕분에 기존 사건들의 확률을 대략적으로 파악할 수 있게 됨으로써 베이즈 정리 활용이 가능해졌다.

</aside>

---

<aside>
💡

**또 다른 이야기 - 머신러닝**

---

- 머신러닝은 데이터 셋이 주어졌을 때, 특정 사건 혹은 가설의 확률을 높여줄 수 있는 최적의 모델을 찾는 것을 목적으로 한다.
- 이는 베이지안 모델이 주어진 정보를 업데이트 해나가면서, 최적의 사후 확률을 계산하는 방식과 일맥 상통한다.
</aside>

### 사전확률과 사후확률

---

$$
P(B|A) = \frac {P(A|B)P(B)} {P(A)}
$$

- **P(A)** : A의 **사전 확률** - 현재의 증거
    - 결과가 나타나기 전에 결정되어 있는 A(원인)의 확률, 현재의 증가
- **P(B)** : B의 **사전 확률** - 과거의 경험
- **P(A|B)** : 원인 B가 주어졌을 때 결과 A의 조건부 확률, **likelihood**
    - 알려진 결과에 기초한 어떤 가설에 대한 가능성
- **P(B|A)** : 사건 A라는 증거에 대한 **사후 확률**
    - 결과 A가 일어났다는 것을 알고, 그때 원인 B가 발생할 확률

## 베이즈 정리의 두 가지 맥락

---

<aside>
1️⃣

**역확률 문제**

---

- P(B|A)를 알고 있을 때, 전제와 관심 사건의 관계가 정 반대인 P(A|B)를 구한다.
</aside>

![확률의 곱셈정리와 전체 확률의 법칙을 이용한다.](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%206.png)

확률의 곱셈정리와 전체 확률의 법칙을 이용한다.

<aside>
💡

**역확률 문제 예시**

---

- 병 A를 앓고 있는지를 판정하는 양성판정 정확도(즉, 병 A가 걸린 사람이 실제로 테스트 결과 양성으로 나올 확률)가 90%인 검사기가 있고, 어떤 사람이 이 검사기로 검사를 시행해서 양성판정이 나왔다면, 이 사람이 90%의 확률로 병에 걸려 있다고 이야기할 수 있을까?
- 그렇지 않다. 왜냐하면 이 90%는 병에 걸린 것이 확실한 사람을 검사했을 때에 양성판정 확률이고(용어로는 "민감도"), 병에 확실히 안걸린 사람을 측정시 음성이라고 나올 확률은(용어로는 "특이도") 따로 분포하므로(심지어 이 검사기가 병에 안 걸린 사람을 병에 걸린 것으로 오판할 확률도 항상 존재하므로).
- 다시 말하자면, 검사가 알려주는 확률과 우리가 알고 싶은 확률은 조건부 확률의 의미에서 정반대이기 때문이다.
</aside>

<aside>
💡

검사의 양성판정 정확도 '90%'는 검사가 병을 가진 사람을 정확하게 포착할 확률, 

즉 **병을 가지고 있다는 전제 하에 검사 결과가 양성일 확률**이 90%임을 의미한다.

</aside>

<aside>
💡

하지만 우리가 알고 싶은 것은 **검사 결과가 양성이라는 전제 하에 병을 앓고 있을 확률**이다. 이는 앞에서 말한 확률과는 그 의미가 전혀 다르다. 

아래 표에서도 볼 수 있듯, 조건부 확률의 관점에서 보면 전제(조건)와 관심 사건의 관계가 정반대이기 때문에, 이런 식의 확률을 구해야 하는 문제를 역확률 문제라고 부른다.

</aside>

|  | **전제** | **관심 사건** | **표현** |
| --- | --- | --- | --- |
| **검사의 정확도** | 병을 앓고 있다 | 검사 결과 : 양성 | P(검사 결과 : 양성 | 병을 앓고 있다) = 0.90 |
| **우리의 관심사** | 검사 결과 : 양성 | 병을 앓고 있다 | P(병을 앓고 있다 | 검사 결과 : 양성) = ???? |

<aside>
💡

원래대로라면 검사의 정확도만을 가지고 우리의 관심사인 '(양성인 사람이) 병을 앓고 있을 확률'을 알 수는 없다. 

하지만 우리가 검사 대상인 질병의 유병률을 알고 있다면, 베이즈 정리를 통해 역확률을 계산할 수 있다. 

예를 들면 전세계 인구 중 1%정도의 사람들이 병 A를 앓는다고 알려져 있다고 가정하자. 

그리고 음성판정 정확도(병 A가 걸리지 않은 사람이 실제로 테스트 결과 음성으로 나올 확률)도 양성판정 정확도와 마찬가지로 90%라고 가정하자 (실제로는 음성판정 정확도가 양성판정 정확도와 같을 필요는 없다). 

그렇다면 검사 결과가 양성으로 나온 사람이 실제로 병 A를 앓고 있을 확률은 약 8.3%이다.

</aside>

$$
P(병∣양성) = 
\frac {P(양성)P(양성∣병)} {P(병)}\\
=\frac {P(양성∣병)P(병)} {P(양성∣병)P(병)+P(양성∣무병)P(무병)} \\
=\frac {0.9×0.01} {0.9×0.01+(1−0.9)×0.99} ≈ 8.3%

$$

### 역확률 문제 예시코드

---

```python
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# 시드 설정 (재현 가능성을 위해)
np.random.seed(42)

# 총 인원수
population_size = 10000

# 사전 확률
P_disease = 0.01  # 병에 걸릴 확률
P_no_disease = 1 - P_disease  # 병에 걸리지 않을 확률

# 조건부 확률
P_positive_given_disease = 0.9  # 병이 있을 때 양성일 확률 (민감도)
P_positive_given_no_disease = 0.1  # 병이 없을 때 양성일 확률 (위양성률)

# 사람마다 병에 걸렸는지 여부를 랜덤하게 결정
disease = np.random.choice([1, 0], size=population_size, p=[P_disease, P_no_disease])

# 검사 결과 생성
test_results = np.where(
    disease == 1,
    np.random.choice([1, 0], size=population_size, p=[P_positive_given_disease, 1 - P_positive_given_disease]),
    np.random.choice([1, 0], size=population_size, p=[P_positive_given_no_disease, 1 - P_positive_given_no_disease])
)

# 데이터프레임 생성
data = pd.DataFrame({
    'Disease': disease,
    'Test_Result': test_results
})

# 양성 판정을 받은 사람들의 데이터 필터링
positive_cases = data[data['Test_Result'] == 1]

# 양성 판정 중 실제로 병에 걸린 비율 계산
prob_disease_given_positive = positive_cases['Disease'].mean()

# 시각화
plt.figure(figsize=(12, 6))

# 1. 전체 인구의 병에 걸린 사람과 걸리지 않은 사람의 분포
plt.subplot(1, 2, 1)
sns.countplot(x='Disease', data=data, palette='viridis')
plt.title('Distribution of Disease in the Population')
plt.xlabel('Disease Status (0 = No Disease, 1 = Disease)')
plt.ylabel('Count')

# 2. 양성 판정을 받은 사람 중 실제로 병에 걸린 사람의 비율
plt.subplot(1, 2, 2)
sns.countplot(x='Disease', data=positive_cases, palette='magma')
plt.title('Distribution of Disease Among Positive Test Results')
plt.xlabel('Disease Status (0 = No Disease, 1 = Disease)')
plt.ylabel('Count')

# 결과 출력
plt.tight_layout()
plt.show()

# 사후 확률 출력
prob_disease_given_positive

```

### 반복적인 검사를 통한 사후확률 업데이트

---

```python
import numpy as np
import matplotlib.pyplot as plt

# 시드 설정
np.random.seed(42)

# 초기 사전 확률
prior_prob = 0.01  # 초기 병에 걸릴 확률 (사전 확률)

# 조건부 확률
P_positive_given_disease = 0.9  # 병이 있을 때 양성일 확률 (민감도)
P_positive_given_no_disease = 0.1  # 병이 없을 때 양성일 확률 (위양성률)

# 반복 횟수
num_tests = 10

# 각 검사에서의 사후 확률을 저장할 리스트
posterior_probs = []

# 반복적으로 사후 확률을 계산
for i in range(num_tests):
    # P(Test Positive) 계산
    P_positive = (P_positive_given_disease * prior_prob) + (P_positive_given_no_disease * (1 - prior_prob))

    # 사후 확률 계산
    posterior_prob = (P_positive_given_disease * prior_prob) / P_positive

    # 사후 확률을 사전 확률로 업데이트
    prior_prob = posterior_prob

    # 결과 저장
    posterior_probs.append(posterior_prob)

# 결과 출력 및 시각화
plt.figure(figsize=(10, 6))
plt.plot(range(1, num_tests + 1), posterior_probs, marker='o', linestyle='-', color='blue')
plt.title('Posterior Probability After Each Test')
plt.xlabel('Test Number')
plt.ylabel('Posterior Probability of Having the Disease')
plt.grid(True)
plt.show()

# 최종 사후 확률
print(posterior_prob)
```

- 양성일 때, 실제 병에 걸렸을 확률을 검사
- 사전 확률을 사후 확률로 계속 업데이트
    - 여러번 검사를 하는 것
- 10번 검사하면 거의 1에 수렴한다.
    - 10번의 검사가 끝난 후 양성이 나왔을 때 실제 병에 걸렸을 확률

<aside>
2️⃣

**데이터를 이용한 사후 확률의 추정**

---

- 이전의 경험과 현재의 증거를 토대로 어떤 사건의 확률을 추론하는 알고리즘
- **어떤 사건이 일어날 확률에 대한 임의의 가정 P(A)에 실제로 발견된 자료나 증거 B를 반영해서, 자료로 미루어보아 어떤 사건이 일어날 확률 P(A|B)을 구하는 것**
</aside>

<aside>
💡

**데이터를 이용한 사후 확률의 추정 예시**

---

1. 처음에는 어떤 사람이 병 A에 걸려있을 확률에 대해 아는 것이 없어, 전 세계 인구 일반이 해당 질병에 걸릴 확률인 1%의 유병률을 가정했다.
2. 그런데 정확도가 90%인 검사를 받았더니 양성 판정을 받았다.
3. **이 사람이 검사에서 양성 판정을 받았다는 새로운 사실을 토대로** 이 사람이 실제로 병에 걸려있을 확률을 알 수 있지 않을까?
</aside>

<aside>
💡

이 문제에서 베이즈 정리가 알려주는 것은, 만약 우리가 '어떤 사건 A가 일어났다고 가정할 때 B라는 자료를 얻게 될 확률'에 대한 정보만 알고 있다면, 자료에 근거해서 어떤 사건이 일어날 확률을 새로 계산할 수 있다는 것이다. 

위의 사례에서는 검사의 정확도가 '어떤 사건(병에 걸림)이 일어났다고 가정할 때 자료(양성 판정을 받음)를 얻게 될 확률'에 대한 정보를 제공해주기 때문에 이를 이용해 검사 결과를 토대로 그 사람이 병에 걸렸을 확률을 새로 계산할 수 있다.

</aside>

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%207.png)

- 새로운 자료가 없는 상태에서 어떤 사건이 일어날 확률에 대한 가정이 필요한데, 이를 사전 확률(prior probability)이라 한다.
- 사건이 일어났다는 가정 하에서 새로이 가지게 된 자료가 관측될 확률을 가능도(likelihood)라고 한다.
- 사전 확률과 가능도를 이용해서 새롭게 계산한, '(새로운 자료로 미루어보아 새롭게 판단한) 어떤 사건이 일어날 확률'을 사후 확률(posterior probability)이라고 한다.
- 베이즈 정리의 분모에 해당하는 부분은 가능도를 구할 때 조건으로 걸린 사건(위의 예의 경우, (실제 병의 유무와는 상관 없이) '양성 판정이 나올 확률')의 확률이다. 기능적으로는 사후 확률이 확률의 정의(0 이상 1 이하여야 한다)를 충족시키도록 사전확률과 가능도의 곱을 보정해주는 역할을 한다. 위와 같은 예에서는 쉽게 계산할 수 있고 엄밀하게 사후확률을 구하려면 반드시 필요한 부분이지만, 실제로 생각보다 계산이 까다로울 경우 등식을 비례 관계로 바꾸고 생략할 수도 있다.

### 예제

---

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%208.png)

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%209.png)

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%2010.png)

![image.png](%E1%84%8C%E1%85%A9%E1%84%80%E1%85%A5%E1%86%AB%E1%84%87%E1%85%AE%20%E1%84%92%E1%85%AA%E1%86%A8%E1%84%85%E1%85%B2%E1%86%AF%20&%20%E1%84%87%E1%85%A6%E1%84%8B%E1%85%B5%E1%84%8C%E1%85%B5%E1%84%8B%E1%85%A1%E1%86%AB%20%E1%84%90%E1%85%A9%E1%86%BC%E1%84%80%E1%85%A8%E1%84%92%E1%85%A1%E1%86%A8%201811f394851c4658976cc454582c2bb0/image%2011.png)

## 베이즈 정리의 활용 - 기계학습

<aside>
💡

**나이브 베이지안**

---

- 베이즈 정리를 이용한 확률적 기계 학습 알고리즘
- 사전 확률에 기반을 두고 사후 확률을 추론하는 확률적 예측
- 이 때, 모든 사건이 **독립사건**이라는 **naive한 가정**을 하고 있기 때문에 **나이브 베이지안**
- 스팸메일 필터, 텍스트 분석기, 추천 시스템 등에서 예측과 추론을 위한 분류기로 활용
</aside>

<aside>
💡

**나이브 베이즈 분류기**

---

- 특성들 사이의 독립적 가정을 기반으로 텍스트 문서를 여러 범주로 분류하는 대표적인 분류기
- 순진한 가정을 하고 있음에도 불구하고, 스팸 필터와 같은 복잡한 문제의 해결에서도 아주 우수한 성능을 보인다.
</aside>

### 별 의미는 없긴한데 iris 분류

---

```python
# 필요한 라이브러리 불러오기
from sklearn.model_selection import train_test_split
from sklearn.naive_bayes import GaussianNB
from sklearn.metrics import accuracy_score, classification_report
from sklearn.datasets import load_iris

# 데이터셋 불러오기 (Iris 데이터셋 사용)
iris = load_iris()
X = iris.data
y = iris.target

# 학습 데이터와 테스트 데이터로 분리
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 나이브 베이즈 분류기 모델 생성
model = GaussianNB()

# 모델 학습
model.fit(X_train, y_train)

# 테스트 데이터로 예측
y_pred = model.predict(X_test)

# 모델 성능 평가
accuracy = accuracy_score(y_test, y_pred)
report = classification_report(y_test, y_pred)

# 결과 출력
print(f"Accuracy: {accuracy:.2f}")
print("Classification Report:")
print(report)
```